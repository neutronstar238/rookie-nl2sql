# LLM Client è®¾è®¡è¯¦è§£

æœ¬æ–‡æ·±å…¥è®²è§£ M1 æ¨¡å—ä¸­çš„ LLM Client å°è£…ï¼Œå¸®åŠ©ä½ ç†è§£å¦‚ä½•æ„å»ºç»Ÿä¸€çš„ã€æ”¯æŒå¤šæä¾›å•†çš„ LLM å®¢æˆ·ç«¯ã€‚

## ä¸ºä»€ä¹ˆéœ€è¦ LLM Clientï¼Ÿ

### é—®é¢˜åœºæ™¯

åœ¨å¼€å‘ NL2SQL ç³»ç»Ÿæ—¶ï¼Œä½ å¯èƒ½é¢ä¸´è¿™äº›é—®é¢˜ï¼š

**é—®é¢˜ 1: å¤šä¸ª LLM æä¾›å•†**
```python
# ä½¿ç”¨ DeepSeek
from openai import OpenAI
client = OpenAI(api_key="sk-xxx", base_url="https://api.deepseek.com")

# åˆ‡æ¢åˆ° Qwen
client = OpenAI(api_key="sk-yyy", base_url="https://dashscope.aliyuncs.com/...")

# åˆ‡æ¢åˆ° OpenAI
client = OpenAI(api_key="sk-zzz")
```

æ¯æ¬¡åˆ‡æ¢éƒ½è¦æ”¹ä»£ç ï¼

**é—®é¢˜ 2: é…ç½®åˆ†æ•£**
```python
# API Key ç¡¬ç¼–ç 
api_key = "sk-xxxxx"  # âŒ ä¸å®‰å…¨

# é…ç½®æ•£è½å„å¤„
model = "deepseek-chat"
temperature = 0.0
max_tokens = 2000
```

**é—®é¢˜ 3: è°ƒç”¨æ¥å£ä¸ç»Ÿä¸€**
```python
# æœ‰æ—¶ç”¨è¿™ç§
response = client.chat.completions.create(...)

# æœ‰æ—¶ç”¨é‚£ç§
response = client.invoke(...)

# è¿˜è¦å¤„ç†ä¸åŒçš„å“åº”æ ¼å¼
```

### LLM Client çš„è§£å†³æ–¹æ¡ˆ

âœ… **ç»Ÿä¸€æ¥å£**
```python
from tools.llm_client import llm_client

# ç®€å•è°ƒç”¨ï¼Œä¸ç®¡ä»€ä¹ˆæä¾›å•†
response = llm_client.chat(prompt="æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·")
```

âœ… **é…ç½®é›†ä¸­ç®¡ç†**
```python
# .env æ–‡ä»¶
LLM_PROVIDER=deepseek
DEEPSEEK_API_KEY=sk-xxxxx

# ä»£ç ä¸­è‡ªåŠ¨è¯»å–
llm_client = LLMClient()  # è‡ªåŠ¨åŠ è½½é…ç½®
```

âœ… **è½»æ¾åˆ‡æ¢æä¾›å•†**
```bash
# åªéœ€æ”¹ .env æ–‡ä»¶
LLM_PROVIDER=qwen
QWEN_API_KEY=sk-yyyyy

# ä»£ç æ— éœ€ä¿®æ”¹ï¼
```

## æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          NL2SQL Application                 â”‚
â”‚  (graphs/nodes/generate_sql.py)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LLM Client (tools/llm_client.py)   â”‚
â”‚  â€¢ Unified Interface                        â”‚
â”‚  â€¢ Provider Abstraction                     â”‚
â”‚  â€¢ Message Formatting                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Configuration (configs/config.py)      â”‚
â”‚  â€¢ Load .env + YAML                         â”‚
â”‚  â€¢ Provider Selection                       â”‚
â”‚  â€¢ Parameter Management                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼           â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚DeepSeek â”‚ â”‚  Qwen   â”‚ â”‚ OpenAI  â”‚
â”‚   API   â”‚ â”‚   API   â”‚ â”‚   API   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è®¾è®¡åŸåˆ™

1. **å•ä¸€èŒè´£**ï¼šLLM Client åªè´Ÿè´£ LLM è°ƒç”¨ï¼Œä¸å¤„ç†ä¸šåŠ¡é€»è¾‘
2. **ä¾èµ–æ³¨å…¥**ï¼šé…ç½®ä»å¤–éƒ¨æ³¨å…¥ï¼Œä¸ç¡¬ç¼–ç 
3. **å¼€é—­åŸåˆ™**ï¼šæ”¯æŒæ‰©å±•æ–°æä¾›å•†ï¼Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒä»£ç 
4. **æ¥å£éš”ç¦»**ï¼šæä¾›ç®€å•çš„ `chat()` æ¥å£ï¼Œéšè—å¤æ‚æ€§

## æ ¸å¿ƒå®ç°

### 1. é…ç½®ç®¡ç†ï¼ˆConfigï¼‰

#### é…ç½®åŠ è½½æµç¨‹

```python
class Config:
    def __init__(self, env: str = "dev"):
        self._load_yaml_config()  # 1. åŠ è½½ YAML
        self._load_env_vars()     # 2. åŠ è½½ç¯å¢ƒå˜é‡
```

**ä¼˜å…ˆçº§**ï¼šç¯å¢ƒå˜é‡ > YAML é…ç½®

#### LLM é…ç½®è·å–

`configs/config.py:108-145`

```python
def get_llm_config(self) -> Dict[str, Any]:
    """æ ¹æ®é€‰æ‹©çš„æä¾›å•†è·å– LLM é…ç½®"""
    provider = self.get("llm_provider", "deepseek").lower()

    config = {
        "provider": provider,
        "temperature": self.get("llm_temperature", 0.0),
        "max_tokens": self.get("llm_max_tokens", 2000),
        "timeout": self.get("llm_timeout", 30),
    }

    if provider == "deepseek":
        config.update({
            "api_key": self.get("deepseek_api_key"),
            "base_url": self.get("deepseek_base_url"),
            "model": self.get("deepseek_model"),
        })
    elif provider == "qwen":
        config.update({
            "api_key": self.get("qwen_api_key"),
            "base_url": self.get("qwen_base_url"),
            "model": self.get("qwen_model"),
        })
    elif provider == "openai":
        config.update({
            "api_key": self.get("openai_api_key"),
            "base_url": self.get("openai_base_url"),
            "model": self.get("openai_model"),
        })

    return config
```

**å…³é”®ç‚¹**ï¼š
- ç»Ÿä¸€çš„é…ç½®ç»“æ„
- æ ¹æ® `provider` åŠ¨æ€é€‰æ‹©å‚æ•°
- æä¾›é»˜è®¤å€¼

### 2. LLM Client å°è£…

#### åˆå§‹åŒ–

`tools/llm_client.py:30-60`

```python
class LLMClient:
    def __init__(self, provider: Optional[str] = None):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        # è·å–é…ç½®
        llm_config = config.get_llm_config()

        self.provider = llm_config["provider"]
        self.model = llm_config["model"]

        # ä½¿ç”¨ LangChain çš„ ChatOpenAI
        # æ‰€æœ‰æä¾›å•†éƒ½å…¼å®¹ OpenAI API æ ¼å¼
        self.client = ChatOpenAI(
            model=llm_config["model"],
            api_key=llm_config["api_key"],
            base_url=llm_config["base_url"],
            temperature=llm_config["temperature"],
            max_tokens=llm_config["max_tokens"],
            timeout=llm_config["timeout"]
        )
```

**ä¸ºä»€ä¹ˆç”¨ ChatOpenAIï¼Ÿ**
- DeepSeekã€Qwen éƒ½å…¼å®¹ OpenAI API æ ¼å¼
- åªéœ€æ”¹ `base_url` å’Œ `api_key`
- ç»Ÿä¸€çš„æ¶ˆæ¯æ ¼å¼ï¼ˆSystemMessage, HumanMessageï¼‰

#### Chat æ–¹æ³•

`tools/llm_client.py:62-101`

```python
def chat(
    self,
    prompt: str,
    system_message: Optional[str] = None,
    **kwargs
) -> str:
    """å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤"""
    messages = []

    # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰
    if system_message:
        messages.append(SystemMessage(content=system_message))

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    messages.append(HumanMessage(content=prompt))

    # è°ƒç”¨ LLM
    response = self.client.invoke(messages)

    # è¿”å›æ–‡æœ¬å†…å®¹
    return response.content
```

**ç®€åŒ–äº†ä»€ä¹ˆï¼Ÿ**

**åŸå§‹æ–¹å¼**ï¼ˆå¤æ‚ï¼‰ï¼š
```python
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com"
)

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯SQLä¸“å®¶"},
        {"role": "user", "content": "æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·"}
    ],
    temperature=0.0,
    max_tokens=2000
)

sql = response.choices[0].message.content
```

**å°è£…å**ï¼ˆç®€å•ï¼‰ï¼š
```python
from tools.llm_client import llm_client

sql = llm_client.chat(
    prompt="æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·",
    system_message="ä½ æ˜¯SQLä¸“å®¶"
)
```

### 3. å…¨å±€å®ä¾‹

`tools/llm_client.py:137-138`

```python
# å…¨å±€ LLM å®¢æˆ·ç«¯å®ä¾‹
llm_client = LLMClient()
```

**ä¼˜åŠ¿**ï¼š
- é¡¹ç›®ä¸­ä»»ä½•åœ°æ–¹éƒ½å¯ä»¥ `from tools.llm_client import llm_client`
- é…ç½®åªåŠ è½½ä¸€æ¬¡
- å‡å°‘åˆå§‹åŒ–å¼€é”€

## ä½¿ç”¨æŒ‡å—

### åŸºç¡€ç”¨æ³•

#### 1. é…ç½® API Key

```bash
# å¤åˆ¶æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘ .env
LLM_PROVIDER=deepseek
DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxx
```

#### 2. ç®€å•è°ƒç”¨

```python
from tools.llm_client import llm_client

# æœ€ç®€å•çš„è°ƒç”¨
response = llm_client.chat("1+1ç­‰äºå¤šå°‘ï¼Ÿ")
print(response)  # "2"
```

#### 3. å¸¦ç³»ç»Ÿæ¶ˆæ¯

```python
response = llm_client.chat(
    prompt="å°†è¿™å¥è¯ç¿»è¯‘æˆSQL: æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·",
    system_message="ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ï¼Œæ“…é•¿SQLç¿»è¯‘"
)
print(response)
# SELECT * FROM users;
```

### é«˜çº§ç”¨æ³•

#### 1. åŠ¨æ€è°ƒæ•´å‚æ•°

```python
# æé«˜åˆ›é€ æ€§ï¼ˆç”¨äºç”Ÿæˆç¤ºä¾‹æ•°æ®ç­‰ï¼‰
response = llm_client.chat(
    prompt="ç”Ÿæˆ5ä¸ªç¤ºä¾‹ç”¨æˆ·å",
    temperature=0.8  # è¦†ç›–é»˜è®¤çš„ 0.0
)

# é™åˆ¶è¾“å‡ºé•¿åº¦
response = llm_client.chat(
    prompt="è§£é‡Šä»€ä¹ˆæ˜¯NL2SQL",
    max_tokens=100  # é™åˆ¶åœ¨ 100 tokens
)
```

#### 2. å¤šè½®å¯¹è¯

```python
messages = [
    {"role": "system", "content": "ä½ æ˜¯SQLä¸“å®¶"},
    {"role": "user", "content": "å¦‚ä½•æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·ï¼Ÿ"},
    {"role": "assistant", "content": "SELECT * FROM users;"},
    {"role": "user", "content": "å¦‚ä½•åªæŸ¥è¯¢åå­—å«å¼ ä¸‰çš„ç”¨æˆ·ï¼Ÿ"}
]

response = llm_client.chat_with_messages(messages)
print(response)
# SELECT * FROM users WHERE name = 'å¼ ä¸‰';
```

#### 3. åˆ‡æ¢æä¾›å•†

**æ–¹å¼ 1ï¼šä¿®æ”¹ .env**
```bash
# ä» DeepSeek åˆ‡æ¢åˆ° Qwen
LLM_PROVIDER=qwen
QWEN_API_KEY=sk-xxxxxxxx
```

**æ–¹å¼ 2ï¼šä»£ç ä¸­ä¸´æ—¶åˆ‡æ¢**
```python
# åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯
qwen_client = LLMClient(provider="qwen")
response = qwen_client.chat("æµ‹è¯•é—®é¢˜")

# å…¨å±€å®¢æˆ·ç«¯ä¸å—å½±å“
deepseek_response = llm_client.chat("æµ‹è¯•é—®é¢˜")
```

### åœ¨ NL2SQL ä¸­çš„ä½¿ç”¨

`graphs/nodes/generate_sql.py:109-116`

```python
def generate_sql_node(state: NL2SQLState) -> NL2SQLState:
    # 1. æ„å»º Prompt
    prompt_template = load_prompt_template("nl2sql")
    prompt = prompt_template.format(
        schema=schema_placeholder,
        question=state["question"]
    )

    # 2. è°ƒç”¨ LLMï¼ˆä½¿ç”¨å…¨å±€ llm_clientï¼‰
    response = llm_client.chat(prompt=prompt)

    # 3. æå– SQL
    sql = extract_sql_from_response(response)

    return {**state, "candidate_sql": sql}
```

**ä¸ºä»€ä¹ˆè¿™ä¹ˆç®€æ´ï¼Ÿ**
- LLM Client éšè—äº†æ‰€æœ‰é…ç½®ç»†èŠ‚
- ä¸ç”¨å…³å¿ƒç”¨çš„æ˜¯å“ªä¸ªæä¾›å•†
- ä¸“æ³¨äºä¸šåŠ¡é€»è¾‘ï¼ˆPrompt æ„å»ºã€SQL æå–ï¼‰

## æ”¯æŒçš„ LLM æä¾›å•†

### 1. DeepSeekï¼ˆæ¨èå›½å†…ç”¨æˆ·ï¼‰

**é…ç½®**ï¼š
```bash
LLM_PROVIDER=deepseek
DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxx
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_MODEL=deepseek-chat
```

**ä¼˜åŠ¿**ï¼š
- âœ… æ€§ä»·æ¯”é«˜ï¼ˆä¾¿å®œï¼‰
- âœ… ä¸­æ–‡èƒ½åŠ›å¼º
- âœ… API å…¼å®¹ OpenAI
- âœ… å›½å†…è®¿é—®å¿«

**è·å– API Key**ï¼š
- æ³¨å†Œï¼šhttps://platform.deepseek.com
- åˆ›å»º API Key
- å……å€¼ï¼ˆ1å…ƒèµ·ï¼‰

### 2. Qwenï¼ˆé˜¿é‡Œäº‘ï¼‰

**é…ç½®**ï¼š
```bash
LLM_PROVIDER=qwen
QWEN_API_KEY=sk-xxxxxxxxxxxxxxxx
QWEN_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
QWEN_MODEL=qwen-plus
```

**æ¨¡å‹é€‰æ‹©**ï¼š
- `qwen-turbo`: å¿«é€Ÿã€ä¾¿å®œï¼ˆé€‚åˆå¼€å‘æµ‹è¯•ï¼‰
- `qwen-plus`: å¹³è¡¡ï¼ˆé€‚åˆç”Ÿäº§ï¼‰
- `qwen-max`: æœ€å¼ºï¼ˆé€‚åˆå¤æ‚ä»»åŠ¡ï¼‰

**ä¼˜åŠ¿**ï¼š
- âœ… é˜¿é‡Œäº‘ç”Ÿæ€
- âœ… ä¸­æ–‡ä¼˜åŒ–
- âœ… ç¨³å®šæ€§é«˜

**è·å– API Key**ï¼š
- å¼€é€šï¼šhttps://dashscope.aliyun.com
- åˆ›å»º API Key

### 3. OpenAI

**é…ç½®**ï¼š
```bash
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxx
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4
```

**æ¨¡å‹é€‰æ‹©**ï¼š
- `gpt-3.5-turbo`: å¿«é€Ÿã€ä¾¿å®œ
- `gpt-4`: å¼ºå¤§ã€è´µ
- `gpt-4-turbo`: å¿«é€Ÿç‰ˆ GPT-4

**ä¼˜åŠ¿**ï¼š
- âœ… æœ€å¼ºå¤§
- âœ… ç”Ÿæ€å®Œå–„

**åŠ£åŠ¿**ï¼š
- âŒ è´µ
- âŒ å›½å†…è®¿é—®éœ€ä»£ç†

## é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯

#### é”™è¯¯ 1: API Key æœªé…ç½®

```
Error: No API key provided
```

**è§£å†³**ï¼š
```bash
# æ£€æŸ¥ .env æ–‡ä»¶
cat .env | grep API_KEY

# ç¡®ä¿å¯¹åº”æä¾›å•†çš„ API Key å·²è®¾ç½®
DEEPSEEK_API_KEY=sk-xxxxxxxx
```

#### é”™è¯¯ 2: ç½‘ç»œè¿æ¥å¤±è´¥

```
Error: Connection timeout
```

**è§£å†³**ï¼š
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. æ£€æŸ¥ `base_url` æ˜¯å¦æ­£ç¡®
3. å¦‚æœæ˜¯ OpenAIï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ä»£ç†

#### é”™è¯¯ 3: æä¾›å•†ä¸æ”¯æŒ

```
ValueError: Unsupported LLM provider: xxx
```

**è§£å†³**ï¼š
```bash
# æ£€æŸ¥ LLM_PROVIDER é…ç½®
# åªæ”¯æŒ: deepseek, qwen, openai
LLM_PROVIDER=deepseek
```

### ä»£ç ä¸­çš„é”™è¯¯å¤„ç†

```python
def generate_sql_node(state: NL2SQLState) -> NL2SQLState:
    try:
        response = llm_client.chat(prompt=prompt)
        sql = extract_sql_from_response(response)

        return {
            **state,
            "candidate_sql": sql,
            "sql_generated_at": datetime.now().isoformat()
        }

    except Exception as e:
        print(f"âœ— Error generating SQL: {e}")

        return {
            **state,
            "candidate_sql": None,  # æ ‡è®°å¤±è´¥
            "sql_generated_at": datetime.now().isoformat()
        }
```

**å»ºè®®**ï¼š
- æ€»æ˜¯ç”¨ try-except åŒ…è£¹ LLM è°ƒç”¨
- è®°å½•é”™è¯¯æ—¥å¿—
- è¿”å›æ˜ç¡®çš„å¤±è´¥çŠ¶æ€ï¼ˆ`None` æˆ–é”™è¯¯ä¿¡æ¯ï¼‰

## æµ‹è¯•ä¸éªŒè¯

### æµ‹è¯• LLM Client

```bash
# è¿è¡Œæµ‹è¯•è„šæœ¬
python tools/llm_client.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
=== LLM Client Test ===

âœ“ LLM Client initialized: deepseek (deepseek-chat)

Current Provider: deepseek
Current Model: deepseek-chat

Testing simple chat...
Response: SELECT * FROM users;

âœ“ Chat test passed

=== Test Complete ===
```

### æµ‹è¯•é…ç½®åŠ è½½

```bash
python configs/config.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
=== NL2SQL é…ç½®æµ‹è¯• ===

ç¯å¢ƒ: dev

ç³»ç»Ÿé…ç½®:
  ç³»ç»Ÿåç§°: Rookie NL2SQL
  ç³»ç»Ÿç‰ˆæœ¬: 0.1.0
  æ—¥å¿—çº§åˆ«: INFO

LLM é…ç½®:
  æä¾›å•†: deepseek
  æ¨¡å‹: deepseek-chat
  Base URL: https://api.deepseek.com
  API Key å·²è®¾ç½®: æ˜¯
  Temperature: 0.0
  Max Tokens: 2000
```

### å•å…ƒæµ‹è¯•ç¤ºä¾‹

```python
import pytest
from tools.llm_client import LLMClient

def test_llm_client_initialization():
    """æµ‹è¯•å®¢æˆ·ç«¯åˆå§‹åŒ–"""
    client = LLMClient()
    assert client.provider in ["deepseek", "qwen", "openai"]
    assert client.model is not None

def test_llm_client_chat():
    """æµ‹è¯•åŸºæœ¬å¯¹è¯"""
    client = LLMClient()
    response = client.chat("1+1=?")
    assert "2" in response

def test_llm_client_with_system_message():
    """æµ‹è¯•ç³»ç»Ÿæ¶ˆæ¯"""
    client = LLMClient()
    response = client.chat(
        prompt="æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·",
        system_message="ä½ æ˜¯SQLä¸“å®¶ï¼Œåªè¿”å›SQLè¯­å¥"
    )
    assert "SELECT" in response.upper()
```

## æœ€ä½³å®è·µ

### 1. ä½¿ç”¨å…¨å±€å®ä¾‹

âœ… **æ¨è**ï¼š
```python
from tools.llm_client import llm_client

response = llm_client.chat("é—®é¢˜")
```

âŒ **ä¸æ¨è**ï¼š
```python
from tools.llm_client import LLMClient

client = LLMClient()  # æ¯æ¬¡éƒ½åˆå§‹åŒ–
response = client.chat("é—®é¢˜")
```

**åŸå› **ï¼šå…¨å±€å®ä¾‹åªåˆå§‹åŒ–ä¸€æ¬¡ï¼ŒèŠ‚çœèµ„æºã€‚

### 2. æ˜ç¡®çš„ç³»ç»Ÿæ¶ˆæ¯

âœ… **æ¨è**ï¼š
```python
response = llm_client.chat(
    prompt="æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·",
    system_message="ä½ æ˜¯SQLä¸“å®¶ã€‚åªè¿”å›SQLè¯­å¥ï¼Œä¸è¦è§£é‡Šã€‚"
)
```

âŒ **ä¸æ¨è**ï¼š
```python
response = llm_client.chat("ä½ æ˜¯SQLä¸“å®¶ï¼ŒæŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·")
```

**åŸå› **ï¼šåŒºåˆ†è§’è‰²å®šä¹‰å’Œä»»åŠ¡æè¿°ï¼ŒPrompt æ›´æ¸…æ™°ã€‚

### 3. æ§åˆ¶æ¸©åº¦å‚æ•°

```python
# ç¡®å®šæ€§ä»»åŠ¡ï¼ˆSQL ç”Ÿæˆï¼‰ï¼štemperature = 0.0
sql = llm_client.chat(
    prompt="æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·",
    temperature=0.0  # ç»“æœç¨³å®š
)

# åˆ›é€ æ€§ä»»åŠ¡ï¼ˆç”Ÿæˆç¤ºä¾‹ï¼‰ï¼štemperature = 0.7-0.9
examples = llm_client.chat(
    prompt="ç”Ÿæˆ3ä¸ªç¤ºä¾‹ç”¨æˆ·å",
    temperature=0.8  # ç»“æœå¤šæ ·
)
```

### 4. é”™è¯¯å¤„ç†

```python
try:
    response = llm_client.chat(prompt)
except Exception as e:
    logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
    # é™çº§ç­–ç•¥
    return default_response
```

### 5. æˆæœ¬æ§åˆ¶

```python
# é™åˆ¶è¾“å‡ºé•¿åº¦
response = llm_client.chat(
    prompt="ç”ŸæˆSQL",
    max_tokens=200  # é¿å…è¶…é•¿è¾“å‡º
)

# ä½¿ç”¨ä¾¿å®œçš„æ¨¡å‹ï¼ˆå¼€å‘é˜¶æ®µï¼‰
# .env ä¸­è®¾ç½®ï¼š
# QWEN_MODEL=qwen-turbo  # è€Œä¸æ˜¯ qwen-max
```

## æ‰©å±•æ–°æä¾›å•†

å¦‚æœä½ æƒ³æ·»åŠ æ–°çš„ LLM æä¾›å•†ï¼ˆä¾‹å¦‚ï¼šè®¯é£æ˜Ÿç«ã€ç™¾åº¦æ–‡å¿ƒï¼‰ï¼Œåªéœ€ä»¥ä¸‹æ­¥éª¤ï¼š

### æ­¥éª¤ 1: æ·»åŠ ç¯å¢ƒå˜é‡

`.env.example`:
```bash
# è®¯é£æ˜Ÿç«
SPARK_API_KEY=your_spark_api_key
SPARK_BASE_URL=https://spark-api.xf-yun.com/v1
SPARK_MODEL=spark-3.0
```

### æ­¥éª¤ 2: æ›´æ–° Config

`configs/config.py`:
```python
def _load_env_vars(self):
    self.env_config = {
        # ... ç°æœ‰é…ç½® ...

        # è®¯é£æ˜Ÿç«
        "spark_api_key": os.getenv("SPARK_API_KEY", ""),
        "spark_base_url": os.getenv("SPARK_BASE_URL", ""),
        "spark_model": os.getenv("SPARK_MODEL", "spark-3.0"),
    }

def get_llm_config(self):
    # ... ç°æœ‰ä»£ç  ...

    elif provider == "spark":
        config.update({
            "api_key": self.get("spark_api_key"),
            "base_url": self.get("spark_base_url"),
            "model": self.get("spark_model"),
        })
```

### æ­¥éª¤ 3: ä½¿ç”¨æ–°æä¾›å•†

```bash
# .env
LLM_PROVIDER=spark
SPARK_API_KEY=sk-xxxxxx
```

**å‰æ**ï¼šæ–°æä¾›å•†å¿…é¡»å…¼å®¹ OpenAI API æ ¼å¼ï¼

## æ€»ç»“

### LLM Client çš„æ ¸å¿ƒä»·å€¼

1. **ç»Ÿä¸€æ¥å£**ï¼šä¸€ä¸ª `chat()` æ–¹æ³•æå®šæ‰€æœ‰ LLM è°ƒç”¨
2. **é…ç½®é›†ä¸­**ï¼šæ‰€æœ‰é…ç½®åœ¨ `.env` å’Œ `dev.yaml` ä¸­
3. **æ˜“äºåˆ‡æ¢**ï¼šæ”¹ä¸€è¡Œé…ç½®å³å¯åˆ‡æ¢æä¾›å•†
4. **ç®€åŒ–ä»£ç **ï¼šä¸šåŠ¡ä»£ç ä¸“æ³¨äºé€»è¾‘ï¼Œä¸å¤„ç† API ç»†èŠ‚

### å…³é”®æ–‡ä»¶

| æ–‡ä»¶ | ä½œç”¨ |
|------|------|
| `tools/llm_client.py` | LLM Client å®ç° |
| `configs/config.py` | é…ç½®åŠ è½½å’Œç®¡ç† |
| `.env` | ç¯å¢ƒå˜é‡ï¼ˆAPI Keysï¼‰ |
| `configs/dev.yaml` | å¼€å‘ç¯å¢ƒé…ç½® |

### ä¸‹ä¸€æ­¥

- ğŸ‘‰ [M1 å®è·µä»»åŠ¡](./tasks.md)
- ğŸ‘‰ [è¿”å› M1 æ¦‚è¿°](./overview.md)
- ğŸ‘‰ [æç¤ºè¯å·¥ç¨‹è¯¦è§£](./prompt-engineering.md)
